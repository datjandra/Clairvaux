{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datjandra/Clairvaux/blob/master/PhireBlast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_zrdPMxVbfi"
      },
      "outputs": [],
      "source": [
        "!pip install 'transformers[torch]'\n",
        "!pip uninstall -y transformers\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyFFMaO-VKOL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "def predict(name, gender, age, conditions):\n",
        "  PERSIST_DIR = \"./storage\"\n",
        "  try:\n",
        "    if not os.path.exists(PERSIST_DIR):\n",
        "      model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", low_cpu_mem_usage=True, trust_remote_code=True)\n",
        "      model.save_pretrained(PERSIST_DIR, from_pt=True)\n",
        "    else:\n",
        "      model = AutoModelForCausalLM.from_pretrained(PERSIST_DIR, torch_dtype=\"auto\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
        "    prompt = \"Instruct: Sample data in FHIR JSON format of {age} year old {gender} patient named {name} with {conditions}.\\nOutput:\\n\"\n",
        "    prompt = prompt.format(age=age, gender=gender, name=name, conditions=conditions)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    model.to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_length=256)\n",
        "    text = tokenizer.batch_decode(outputs)[0]\n",
        "    return text\n",
        "  finally:\n",
        "    del model\n",
        "    del tokenizer\n",
        "\n",
        "demo = gr.Blocks()\n",
        "with demo:\n",
        "  gr.Markdown(\"<div class='pull-left'><img width='100' src='https://raw.githubusercontent.com/datjandra/PhireBlast/main/phireblast.png'></div><h3>PhireBlast</h3>\")\n",
        "  with gr.Row():\n",
        "    name = gr.Textbox(label=\"Name\")\n",
        "    gender = gr.Dropdown([\"male\", \"female\"], label=\"Gender\", value=\"female\")\n",
        "  with gr.Row():\n",
        "    age = gr.Textbox(label=\"Age\")\n",
        "    conditions = gr.Textbox(label=\"Conditions\")\n",
        "\n",
        "  output = gr.Textbox(label=\"Data\", lines=10)\n",
        "  submit_button = gr.Button(\"Submit\")\n",
        "  submit_button.click(predict, inputs=[name, gender, age, conditions], outputs=output)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlDVQiNCqsww"
      },
      "outputs": [],
      "source": [
        "# run if needed to clean up memory\n",
        "%reset -f"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPru3omfZNc4FxBIqeGmSlr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}